{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6196d99",
   "metadata": {},
   "source": [
    "# Multimodal Search Engine: Data Processing Pipeline\n",
    "\n",
    "This notebook is designed to run on **Google Colab** to leverage GPU acceleration for processing images and text.\n",
    "\n",
    "**Objective:**\n",
    "1.  Download the Unsplash Lite dataset.\n",
    "2.  Generate embeddings for images using **two models**:\n",
    "    *   **SIGLIP**: For text-to-image semantic search.\n",
    "    *   **DINOv2**: For image-to-image visual similarity search.\n",
    "3.  Store the embeddings and metadata in a **Qdrant** vector database with two collections:\n",
    "    *   `text_visual_index` (SIGLIP)\n",
    "    *   `pure_visual_index` (DINOv2)\n",
    "4.  Save the Qdrant storage to Google Drive for later use in the backend application.\n",
    "\n",
    "**Important:**\n",
    "After running this notebook, you will have a `qdrant_storage` folder (zipped as `qdrant_checkpoint.zip`) in your Google Drive. You should download this file and extract it into your project's `backend/` directory to serve the search engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ffae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.colab.drive as drive\n",
    "\n",
    "# Mount Google Drive to save checkpoints and the final database\n",
    "drive.mount(\"./drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a2822",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "Install necessary libraries and download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b743b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install qdrant-client sentence-transformers pandas tqdm -q\n",
    "\n",
    "# Download Unsplash Lite Dataset\n",
    "!curl -O https://unsplash-datasets.s3.amazonaws.com/lite/latest/unsplash-research-dataset-lite-latest.zip\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip -d ./unsplash-research-dataset-lite-latest ./unsplash-research-dataset-lite-latest.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392969ef",
   "metadata": {},
   "source": [
    "## 2. Load Data & Model\n",
    "Load the dataset metadata into Pandas and initialize the SIGLIP model for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from transformers import AutoProcessor, AutoModel, AutoImageProcessor\n",
    "import torch\n",
    "\n",
    "# --- Load Dataset Metadata ---\n",
    "path = './unsplash-research-dataset-lite-latest/'\n",
    "documents = ['photos', 'keywords', 'collections', 'conversions', 'colors']\n",
    "datasets = {}\n",
    "\n",
    "# Read the photos CSV\n",
    "files = glob.glob(path + documents[0] + \".csv*\")\n",
    "subsets = []\n",
    "for filename in files:\n",
    "    df = pd.read_csv(filename, sep='\\t', header=0)\n",
    "    subsets.append(df)\n",
    "\n",
    "datasets[documents[0]] = pd.concat(subsets, axis=0, ignore_index=True)\n",
    "print(f\"Loaded {len(datasets['photos'])} photo records.\")\n",
    "\n",
    "# --- Load Models ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 1. SIGLIP (Text-to-Image)\n",
    "print(\"Loading SIGLIP model...\")\n",
    "siglip_model = AutoModel.from_pretrained(\"google/siglip-base-patch16-256\").to(device)\n",
    "siglip_processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-256\")\n",
    "siglip_dim = siglip_model.config.vision_config.hidden_size\n",
    "\n",
    "# 2. DINOv2 (Image-to-Image)\n",
    "print(\"Loading DINOv2 model...\")\n",
    "dino_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-large')\n",
    "dino_model = AutoModel.from_pretrained('facebook/dinov2-large').to(device)\n",
    "dino_dim = dino_model.config.hidden_size\n",
    "\n",
    "print(f\"Models loaded.\")\n",
    "print(f\"SIGLIP Dimension: {siglip_dim}\")\n",
    "print(f\"DINOv2 Dimension: {dino_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a6d3a8",
   "metadata": {},
   "source": [
    "## 3. Initialize Vector Database\n",
    "Set up Qdrant and define helper functions for checkpointing (saving progress to Drive) and downloading images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "from qdrant_client import QdrantClient, models\n",
    "import shutil\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# --- Configuration ---\n",
    "QDRANT_PATH = \"./qdrant_storage\"\n",
    "DRIVE_BACKUP_PATH = \"/content/drive/My Drive/qdrant_backups\"\n",
    "CHECKPOINT_ZIP_FILE = \"qdrant_checkpoint.zip\"\n",
    "DRIVE_BACKUP_FILE = os.path.join(DRIVE_BACKUP_PATH, CHECKPOINT_ZIP_FILE)\n",
    "TEMP_ZIP_PATH = \"./qdrant_checkpoint_temp\"\n",
    "\n",
    "COLLECTION_SIGLIP = \"text_visual_index\"\n",
    "COLLECTION_DINO = \"pure_visual_index\"\n",
    "\n",
    "# Ensure backup directory exists\n",
    "os.makedirs(DRIVE_BACKUP_PATH, exist_ok=True)\n",
    "\n",
    "# --- Helper: Save Checkpoint ---\n",
    "def save_checkpoint(client, drive_path):\n",
    "    tqdm.write(f\"\\n--- Checkpointing: Saving database to {drive_path} ---\")\n",
    "    try:\n",
    "        client.close()\n",
    "        shutil.make_archive(base_name=TEMP_ZIP_PATH, format='zip', root_dir=\".\", base_dir=QDRANT_PATH)\n",
    "        shutil.move(f\"{TEMP_ZIP_PATH}.zip\", drive_path)\n",
    "        tqdm.write(f\"--- Checkpoint successful: {CHECKPOINT_ZIP_FILE} updated in Drive. ---\")\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[Warning] Checkpoint failed: {e}\")\n",
    "    finally:\n",
    "        return QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# --- Helper: Download Image ---\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "TOTAL_DOWNLOAD_TIMEOUT = 15.0\n",
    "\n",
    "async def download_image(session, row, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(row['photo_image_url'], timeout=5.0, headers=HEADERS) as response:\n",
    "                response.raise_for_status()\n",
    "                content = await response.read()\n",
    "                img = Image.open(BytesIO(content)).convert(\"RGB\")\n",
    "                return (img, row)\n",
    "        except Exception as e:\n",
    "            return (None, row)\n",
    "\n",
    "# --- Restore or Initialize Database ---\n",
    "print(\"--- Initializing Database ---\")\n",
    "start_index = 0\n",
    "\n",
    "if os.path.exists(DRIVE_BACKUP_FILE):\n",
    "    print(f\"Found checkpoint at {DRIVE_BACKUP_FILE}. Restoring...\")\n",
    "    shutil.rmtree(QDRANT_PATH, ignore_errors=True)\n",
    "    shutil.copy(DRIVE_BACKUP_FILE, \"local_checkpoint.zip\")\n",
    "    shutil.unpack_archive(\"local_checkpoint.zip\", \".\")\n",
    "    os.remove(\"local_checkpoint.zip\")\n",
    "    print(\"Restore complete.\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting fresh.\")\n",
    "\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# 1. Setup SIGLIP Collection\n",
    "if client.collection_exists(COLLECTION_SIGLIP):\n",
    "    info = client.get_collection(COLLECTION_SIGLIP)\n",
    "    start_index = info.points_count\n",
    "    print(f\"Resuming SIGLIP collection from index: {start_index}\")\n",
    "else:\n",
    "    print(f\"Creating collection '{COLLECTION_SIGLIP}'...\")\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_SIGLIP,\n",
    "        vectors_config=models.VectorParams(size=siglip_dim, distance=models.Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# 2. Setup DINOv2 Collection\n",
    "if client.collection_exists(COLLECTION_DINO):\n",
    "    info = client.get_collection(COLLECTION_DINO)\n",
    "    print(f\"DINOv2 collection found with {info.points_count} points.\")\n",
    "else:\n",
    "    print(f\"Creating collection '{COLLECTION_DINO}'...\")\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_DINO,\n",
    "        vectors_config=models.VectorParams(size=dino_dim, distance=models.Distance.COSINE)\n",
    "    )\n",
    "\n",
    "print(\"Database initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08093b97",
   "metadata": {},
   "source": [
    "## 4. Ingestion Pipeline\n",
    "This loop processes images in batches:\n",
    "1.  Downloads images concurrently.\n",
    "2.  Encodes them using SIGLIP (on GPU).\n",
    "3.  Upserts vectors and metadata to Qdrant.\n",
    "4.  Periodically saves checkpoints to Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfd4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCURRENCY_LIMIT = 10\n",
    "BATCH_SIZE = 64\n",
    "CHECKPOINT_EVERY_N_BATCHES = 5\n",
    "batch_counter = 0\n",
    "\n",
    "total_remaining = len(datasets[\"photos\"]) - start_index\n",
    "num_batches = (total_remaining // BATCH_SIZE) + 1\n",
    "\n",
    "print(f\"Starting ingestion from index {start_index}. ({total_remaining} images remaining)\")\n",
    "\n",
    "main_progress_bar = tqdm(range(start_index, len(datasets[\"photos\"]), BATCH_SIZE), total=num_batches, desc=\"Total Batches\")\n",
    "\n",
    "async def ingestion_loop():\n",
    "    global client, batch_counter\n",
    "    semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        for i in main_progress_bar:\n",
    "            batch_df = datasets[\"photos\"].iloc[i:i + BATCH_SIZE]\n",
    "            if batch_df.empty: continue\n",
    "\n",
    "            # 1. Download\n",
    "            tasks = [download_image(session, row, semaphore) for _, row in batch_df.iterrows()]\n",
    "            \n",
    "            images_to_process = []\n",
    "            rows_to_process = []\n",
    "            \n",
    "            for future in asyncio.as_completed(tasks):\n",
    "                img, row = await future\n",
    "                if img:\n",
    "                    images_to_process.append(img)\n",
    "                    rows_to_process.append(row)\n",
    "\n",
    "            if not images_to_process: continue\n",
    "\n",
    "            # 2. Encode with SIGLIP\n",
    "            with torch.no_grad():\n",
    "                inputs_siglip = siglip_processor(images=images_to_process, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "                vectors_siglip = siglip_model.get_image_features(**inputs_siglip).cpu().numpy()\n",
    "\n",
    "            # 3. Encode with DINOv2\n",
    "            with torch.no_grad():\n",
    "                inputs_dino = dino_processor(images=images_to_process, return_tensors=\"pt\").to(device)\n",
    "                outputs_dino = dino_model(**inputs_dino)\n",
    "                # Average pool the last hidden state\n",
    "                vectors_dino = outputs_dino.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "            # 4. Prepare Points\n",
    "            points_siglip = []\n",
    "            points_dino = []\n",
    "            \n",
    "            for idx, row in enumerate(rows_to_process):\n",
    "                payload = {\n",
    "                    \"url\": row['photo_image_url'],\n",
    "                    \"description\": str(row['photo_description']),\n",
    "                    \"unsplash_id\": row['photo_id']\n",
    "                }\n",
    "                \n",
    "                # SIGLIP Point\n",
    "                points_siglip.append(models.PointStruct(\n",
    "                    id=row.name, \n",
    "                    vector=vectors_siglip[idx].tolist(), \n",
    "                    payload=payload\n",
    "                ))\n",
    "                \n",
    "                # DINOv2 Point (Same ID, Same Payload)\n",
    "                points_dino.append(models.PointStruct(\n",
    "                    id=row.name, \n",
    "                    vector=vectors_dino[idx].tolist(), \n",
    "                    payload=payload\n",
    "                ))\n",
    "\n",
    "            # 5. Upsert to Both Collections\n",
    "            if points_siglip:\n",
    "                client.upsert(collection_name=COLLECTION_SIGLIP, points=points_siglip, wait=False)\n",
    "            \n",
    "            if points_dino:\n",
    "                client.upsert(collection_name=COLLECTION_DINO, points=points_dino, wait=False)\n",
    "\n",
    "            batch_counter += 1\n",
    "\n",
    "            # 6. Checkpoint\n",
    "            if batch_counter % CHECKPOINT_EVERY_N_BATCHES == 0:\n",
    "                client = save_checkpoint(client, DRIVE_BACKUP_FILE)\n",
    "\n",
    "# Run the loop\n",
    "await ingestion_loop()\n",
    "\n",
    "# Final Save\n",
    "print(\"Ingestion finished. Saving final checkpoint...\")\n",
    "client = save_checkpoint(client, DRIVE_BACKUP_FILE)\n",
    "client.close()\n",
    "print(f\"Database saved to: {DRIVE_BACKUP_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c046ce61",
   "metadata": {},
   "source": [
    "## 5. Test Search\n",
    "Verify the database works by running a text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b20cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test Script ---\n",
    "search_query = \"a lion facing the camera\"\n",
    "\n",
    "print(f\"Loading database from {QDRANT_PATH}...\")\n",
    "client = QdrantClient(path=QDRANT_PATH)\n",
    "\n",
    "# Verify Counts\n",
    "info_siglip = client.get_collection(COLLECTION_SIGLIP)\n",
    "info_dino = client.get_collection(COLLECTION_DINO)\n",
    "print(f\"SIGLIP Collection Count: {info_siglip.points_count}\")\n",
    "print(f\"DINOv2 Collection Count: {info_dino.points_count}\")\n",
    "\n",
    "# Test Text Search (SIGLIP)\n",
    "print(f\"\\nEncoding query: '{search_query}'\")\n",
    "with torch.no_grad():\n",
    "    inputs = siglip_processor(text=[search_query], padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "    query_vector = siglip_model.get_text_features(**inputs).cpu().numpy()[0]\n",
    "\n",
    "print(\"Searching SIGLIP collection...\")\n",
    "search_results = client.search(collection_name=COLLECTION_SIGLIP, query_vector=query_vector, limit=5)\n",
    "\n",
    "print(\"\\n--- TOP 5 RESULTS (Text-to-Image) ---\")\n",
    "for i, result in enumerate(search_results):\n",
    "    print(f\"\\nResult {i+1} (Score: {result.score:.4f}):\")\n",
    "    if result.payload:\n",
    "        print(f\"  URL: {result.payload.get('url')}\")\n",
    "        print(f\"  Desc: {result.payload.get('description')}\")\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caecf06c",
   "metadata": {},
   "source": [
    "## 6. Test Image-to-Image Search (Visual Similarity)\n",
    "Now we test the DINOv2 index. We will take an image and find other images that look visually similar (shapes, colors, composition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0929c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 1. Pick a query image\n",
    "# We'll just take the first image from our dataframe as the \"query\"\n",
    "query_row = datasets['photos'].iloc[0]\n",
    "query_image_url = query_row['photo_image_url']\n",
    "\n",
    "print(f\"Query Image URL: {query_image_url}\")\n",
    "print(f\"Description: {query_row['photo_description']}\")\n",
    "\n",
    "# 2. Download the image\n",
    "response = requests.get(query_image_url, headers=HEADERS)\n",
    "query_img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "# 3. Encode with DINOv2\n",
    "print(\"Encoding query image with DINOv2...\")\n",
    "with torch.no_grad():\n",
    "    inputs = dino_processor(images=query_img, return_tensors=\"pt\").to(device)\n",
    "    outputs = dino_model(**inputs)\n",
    "    # Average pool to get the embedding\n",
    "    query_vector_dino = outputs.last_hidden_state.mean(dim=1).cpu().numpy()[0]\n",
    "\n",
    "# 4. Search the 'pure_visual_index'\n",
    "print(\"Searching DINOv2 collection...\")\n",
    "client = QdrantClient(path=QDRANT_PATH) # Re-open client just in case\n",
    "search_results_visual = client.search(\n",
    "    collection_name=COLLECTION_DINO,\n",
    "    query_vector=query_vector_dino,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "# 5. Display Results\n",
    "print(\"\\n--- TOP 5 VISUALLY SIMILAR IMAGES ---\")\n",
    "for i, result in enumerate(search_results_visual):\n",
    "    print(f\"\\nResult {i+1} (Score: {result.score:.4f}):\")\n",
    "    if result.payload:\n",
    "        print(f\"  URL: {result.payload.get('url')}\")\n",
    "        # print(f\"  Desc: {result.payload.get('description')}\")\n",
    "\n",
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
